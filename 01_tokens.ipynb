{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe89cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0db6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "thinking understanding information embedding thinking token input model neuron thinking input token model learning output data reasoning network thinking embedding understanding parameter learning system pattern embedding intelligence reasoning data parameter parameter embedding token output learnin ...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a simulated document (200 words from a sample vocabulary)\n",
    "vocab_pool = [\"intelligence\", \"learning\", \"thinking\", \"system\", \"model\", \"language\",\n",
    "              \"neuron\", \"data\", \"pattern\", \"understanding\", \"train\", \"token\", \"input\",\n",
    "              \"output\", \"network\", \"parameter\", \"representation\", \"information\",\n",
    "              \"reasoning\", \"embedding\"]\n",
    "\n",
    "document = \" \".join(random.choices(vocab_pool, k=200))\n",
    "print(\"Sample document:\")\n",
    "print(document[:300], \"...\")  # Preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9c529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary (word -> token ID):\n",
      "data -> 0\n",
      "embedding -> 1\n",
      "information -> 2\n",
      "input -> 3\n",
      "intelligence -> 4\n",
      "language -> 5\n",
      "learning -> 6\n",
      "model -> 7\n",
      "network -> 8\n",
      "neuron -> 9\n",
      "output -> 10\n",
      "parameter -> 11\n",
      "pattern -> 12\n",
      "reasoning -> 13\n",
      "representation -> 14\n",
      "system -> 15\n",
      "thinking -> 16\n",
      "token -> 17\n",
      "train -> 18\n",
      "understanding -> 19\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create vocabulary by counting words (basic word-level tokenization)\n",
    "words = document.split()\n",
    "vocabulary = sorted(set(words))  # sorted just for visual clarity\n",
    "word_to_token = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(\"\\nVocabulary (word -> token ID):\")\n",
    "for word, idx in word_to_token.items():\n",
    "    print(f\"{word} -> {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d694aa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized document (first 50 tokens):\n",
      "[16, 19, 2, 1, 16, 17, 3, 7, 9, 16, 3, 17, 7, 6, 10, 0, 13, 8, 16, 1, 19, 11, 6, 15, 12, 1, 4, 13, 0, 11, 11, 1, 17, 10, 6, 16, 10, 11, 12, 17, 16, 6, 6, 18, 19, 9, 12, 6, 14, 12]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenize the document (convert to token IDs)\n",
    "tokenized = [word_to_token[word] for word in words]\n",
    "print(\"\\nTokenized document (first 50 tokens):\")\n",
    "print(tokenized[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585b0f3",
   "metadata": {},
   "source": [
    "# A) Tokenizing with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4295637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      " What is intelligence? Can machines think or understand meaning?\n",
      "\n",
      "Tokens (IDs):\n",
      " [3923, 374, 11478, 30, 3053, 12933, 1781, 477, 3619, 7438, 30]\n",
      "\n",
      "Back to Text:\n",
      " What is intelligence? Can machines think or understand meaning?\n",
      "\n",
      "Number of tokens: 11\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Step 1: Define some text\n",
    "text = \"What is intelligence? Can machines think or understand meaning?\"\n",
    "\n",
    "# Step 2: Use GPT-3.5 tokenizer\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # tokenizer used in GPT-4/3.5\n",
    "\n",
    "tokens = enc.encode(text)\n",
    "decoded = enc.decode(tokens)\n",
    "\n",
    "print(\"\\nOriginal Text:\\n\", text)\n",
    "print(\"\\nTokens (IDs):\\n\", tokens)\n",
    "print(\"\\nBack to Text:\\n\", decoded)\n",
    "print(\"\\nNumber of tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c218747",
   "metadata": {},
   "source": [
    "# B) Tockenizing with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2afbc683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksquare/.pyenv/versions/3.11.10/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      " What is intelligence? Can machines think or understand meaning?\n",
      "\n",
      "Tokenizer Output:\n",
      "What -> 2061\n",
      "Ġis -> 318\n",
      "Ġintelligence -> 4430\n",
      "? -> 30\n",
      "ĠCan -> 1680\n",
      "Ġmachines -> 8217\n",
      "Ġthink -> 892\n",
      "Ġor -> 393\n",
      "Ġunderstand -> 1833\n",
      "Ġmeaning -> 3616\n",
      "? -> 30\n",
      "\n",
      "Total tokens: 11\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer (same used by GPT-like models)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"What is intelligence? Can machines think or understand meaning?\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"\\nOriginal Text:\\n\", text)\n",
    "print(\"\\nTokenizer Output:\")\n",
    "for tok, tid in zip(tokens, token_ids):\n",
    "    print(f\"{tok} -> {tid}\")\n",
    "\n",
    "print(\"\\nTotal tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927079d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
